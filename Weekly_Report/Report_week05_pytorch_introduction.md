# B√°o C√°o Lab 5: Gi·ªõi Thi·ªáu PyTorch v√† C∆° B·∫£n v·ªÅ Tensor & Autograd üìä

B√°o c√°o n√†y tr√¨nh b√†y k·∫øt qu·∫£ th·ª±c thi v√† ph√¢n t√≠ch c√°c b√†i t·∫≠p c∆° b·∫£n trong PyTorch, bao g·ªìm thao t√°c Tensor, t·ª± ƒë·ªông t√≠nh ƒë·∫°o h√†m (Autograd) v√† x√¢y d·ª±ng ki·∫øn tr√∫c m·∫°ng n∆°-ron ƒë∆°n gi·∫£n (`torch.nn`).

## I. Gi·∫£i Th√≠ch C√°c B∆∞·ªõc Tri·ªÉn Khai (Implementation Steps)

C√°c b∆∞·ªõc tri·ªÉn khai ƒë∆∞·ª£c th·ª±c hi·ªán th√¥ng qua file `lab5-pytorch-introduction.ipynb`, t·∫≠p trung v√†o c√°c kh√°i ni·ªám c·ªët l√µi c·ªßa PyTorch.

### 1. Ph·∫ßn 1: Thao t√°c c∆° b·∫£n v·ªõi Tensor

* **Task 1.1 (T·∫°o Tensor):** Tensor ƒë∆∞·ª£c t·∫°o t·ª´ danh s√°ch Python (`list`) v√† m·∫£ng NumPy (`numpy.array`) b·∫±ng `torch.tensor()` v√† `torch.from_numpy()`. ƒê·ªìng th·ªùi, ki·ªÉm tra c√°c thu·ªôc t√≠nh c∆° b·∫£n nh∆∞ `shape`, `dtype`, v√† `device`.
* **Task 1.2 (Ph√©p to√°n):** Th·ª±c hi·ªán c√°c ph√©p to√°n c∆° b·∫£n nh∆∞ c·ªông ph·∫ßn t·ª≠ (`+`), nh√¢n v√¥ h∆∞·ªõng (`*`), v√† nh√¢n ma tr·∫≠n (`@`) v·ªõi to√°n t·ª≠ chuy·ªÉn v·ªã (`.T`).
* **Task 1.3 (Indexing & Slicing):** Truy c·∫≠p c√°c ph·∫ßn t·ª≠, h√†ng, v√† c·ªôt c·ªßa Tensor b·∫±ng c√∫ ph√°p slicing ti√™u chu·∫©n c·ªßa Python (v√≠ d·ª•: `x_data[0]`, `x_data[:, 1]`).
* **Task 1.4 (Thay ƒë·ªïi h√¨nh d·∫°ng):** S·ª≠ d·ª•ng `tensor.reshape(16, 1)` ƒë·ªÉ thay ƒë·ªïi h√¨nh d·∫°ng c·ªßa Tensor $4 \times 4$ th√†nh $16 \times 1$.

### 2. Ph·∫ßn 2: T·ª± ƒë·ªông t√≠nh ƒê·∫°o h√†m (Autograd)

* **Task 2.1 (Th·ª±c h√†nh):** Kh·ªüi t·∫°o Tensor `x` v·ªõi `requires_grad=True` ƒë·ªÉ PyTorch theo d√µi c√°c ph√©p to√°n. T√≠nh to√°n bi·∫øn ph·ª• thu·ªôc $z = 3(x+2)^2$. G·ªçi `z.backward()` ƒë·ªÉ t√≠nh ƒë·∫°o h√†m $\frac{\partial z}{\partial x}$ v√† ki·ªÉm tra k·∫øt qu·∫£ t·∫°i `x.grad`.

### 3. Ph·∫ßn 3: X√¢y d·ª±ng M√¥ h√¨nh v·ªõi `torch.nn`

* **Task 3.1 (nn.Linear):** Kh·ªüi t·∫°o v√† ki·ªÉm tra l·ªõp tuy·∫øn t√≠nh (`nn.Linear`) ƒë·ªÉ x√°c nh·∫≠n s·ª± thay ƒë·ªïi h√¨nh d·∫°ng ƒë·∫ßu v√†o (input) $(3, 5)$ th√†nh ƒë·∫ßu ra (output) $(3, 2)$.
* **Task 3.2 (nn.Embedding):** Kh·ªüi t·∫°o v√† ki·ªÉm tra l·ªõp nh√∫ng (`nn.Embedding`) ƒë·ªÉ √°nh x·∫° c√°c ch·ªâ s·ªë s·ªë nguy√™n (`input_indices`) th√†nh c√°c vector nh√∫ng d√†y ƒë·∫∑c (dense vectors).
* **Task 3.3 (nn.Module):** X√¢y d·ª±ng l·ªõp **`MyFirstModel`** k·∫ø th·ª´a t·ª´ `nn.Module`, k·∫øt h·ª£p c√°c l·ªõp `nn.Embedding`, `nn.Linear`, v√† h√†m k√≠ch ho·∫°t `nn.ReLU()` ƒë·ªÉ t·∫°o ra m·ªôt ki·∫øn tr√∫c m·∫°ng n∆°-ron c∆° b·∫£n.

---

## II. H∆∞·ªõng D·∫´n Th·ª±c Thi M√£ (Code Execution Guide)

C√°c b∆∞·ªõc th·ª±c thi ƒë∆∞·ª£c th·ª±c hi·ªán trong m√¥i tr∆∞·ªùng Jupyter/IPython.

1.  **M√¥i tr∆∞·ªùng:** ƒê·∫£m b·∫£o th∆∞ vi·ªán **PyTorch** (`torch`), **NumPy** (`numpy`) ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t.
2.  **Th·ª±c thi:** M·ªü file `lab5-pytorch-introduction.ipynb` trong Jupyter Notebook ho·∫∑c m√¥i tr∆∞·ªùng t∆∞∆°ng th√≠ch (nh∆∞ Google Colab/Kaggle).
3.  **Xem K·∫øt qu·∫£:** Ch·∫°y t·ª´ng √¥ code theo th·ª© t·ª±. K·∫øt qu·∫£ (bao g·ªìm h√¨nh d·∫°ng, ki·ªÉu d·ªØ li·ªáu, k·∫øt qu·∫£ ph√©p to√°n, ƒë·∫°o h√†m v√† c·∫•u tr√∫c m√¥ h√¨nh) s·∫Ω ƒë∆∞·ª£c in ra ngay sau m·ªói √¥.

---

## III. Ph√¢n T√≠ch K·∫øt qu·∫£ v√† Gi·∫£i quy·∫øt Th√°ch th·ª©c

### 1. Ph√¢n t√≠ch Task 2.1: Autograd

* **Bi·ªÉu th·ª©c t√≠nh to√°n:** $z = 3(x+2)^2$
* **Gi√° tr·ªã t·∫°i $x=1$:** $y = 1+2 = 3$; $z = 3 \times 3^2 = 27$.
* **T√≠nh ƒë·∫°o h√†m:** $\frac{\partial z}{\partial x} = 6(x+2)$
* **Gi√° tr·ªã ƒë·∫°o h√†m t·∫°i $x=1$:** $\frac{\partial z}{\partial x} = 6(1+2) = 18$.
* **K·∫øt qu·∫£:** `x.grad` hi·ªÉn th·ªã gi√° tr·ªã **`tensor([18.])`**, x√°c nh·∫≠n t√≠nh to√°n ƒë·∫°o h√†m t·ª± ƒë·ªông c·ªßa PyTorch l√† ch√≠nh x√°c.

### 2. Tr·∫£ l·ªùi C√¢u h·ªèi Task 2.1 (Backward() l·∫ßn 2)

* **V·∫•n ƒë·ªÅ:** Khi g·ªçi `z.backward()` l·∫ßn n·ªØa.
* **K·∫øt qu·∫£:** S·∫Ω g√¢y ra l·ªói `RuntimeError: grad can be implicitly created only for scalar outputs` (Ho·∫∑c l·ªói v·ªÅ vi·ªác bi·ªÉu ƒë·ªì ƒë√£ b·ªã gi·∫£i ph√≥ng).
* **Ph√¢n t√≠ch:** PyTorch s·ª≠ d·ª•ng **bi·ªÉu ƒë·ªì t√≠nh to√°n ƒë·ªông** (dynamic computation graph). Sau khi t√≠nh to√°n ƒë·∫°o h√†m ng∆∞·ª£c (`backward()`) l·∫ßn ƒë·∫ßu ti√™n, bi·ªÉu ƒë·ªì m·∫∑c ƒë·ªãnh b·ªã **x√≥a** ƒë·ªÉ gi·∫£i ph√≥ng b·ªô nh·ªõ. Do ƒë√≥, l·∫ßn g·ªçi th·ª© hai th·∫•t b·∫°i v√¨ bi·ªÉu ƒë·ªì c·∫ßn thi·∫øt kh√¥ng c√≤n t·ªìn t·∫°i.

### 3. Ph√¢n t√≠ch C·∫•u tr√∫c M√¥ h√¨nh (Task 3.3)

| L·ªõp | Ch·ª©c nƒÉng | Input Shape (V√≠ d·ª•) | Output Shape (V√≠ d·ª•) |
| :--- | :--- | :--- | :--- |
| **`nn.Embedding`** | √Ånh x·∫° ch·ªâ s·ªë th√†nh vector 16 chi·ªÅu. | $(1, 4)$ (Batch size, Sequence length) | $(1, 4, 16)$ |
| **`nn.Linear`** (linear) | Chuy·ªÉn ƒë·ªïi $16 \to 8$ chi·ªÅu. | $(1, 4, 16)$ | $(1, 4, 8)$ |
| **`nn.ReLU`** | H√†m k√≠ch ho·∫°t phi tuy·∫øn. | $(1, 4, 8)$ | $(1, 4, 8)$ |
| **`nn.Linear`** (output) | Chuy·ªÉn ƒë·ªïi $8 \to 2$ chi·ªÅu (Output layer). | $(1, 4, 8)$ | $(1, 4, 2)$ |

* **K·∫øt qu·∫£:** M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng th√†nh c√¥ng, x√°c nh·∫≠n k√≠ch th∆∞·ªõc Tensor ƒë·∫ßu ra $(1, 4, 2)$ l√† ch√≠nh x√°c, n∆°i $1$ l√† batch size, $4$ l√† sequence length, v√† $2$ l√† output dimension.

---

## IV. Th√°ch th·ª©c v√† Gi·∫£i ph√°p

| Th√°ch th·ª©c | Gi·∫£i ph√°p |
| :--- | :--- |
| **Ki·ªÉu d·ªØ li·ªáu Tensor** | **V·∫•n ƒë·ªÅ:** L·ªõp `nn.Embedding` y√™u c·∫ßu ƒë·∫ßu v√†o l√† Tensor ki·ªÉu `LongTensor` (s·ªë nguy√™n) cho c√°c ch·ªâ s·ªë. | **Gi·∫£i ph√°p:** S·ª≠ d·ª•ng `torch.LongTensor([1, 5, 0, 8])` ƒë·ªÉ kh·ªüi t·∫°o ch√≠nh x√°c c√°c ch·ªâ s·ªë ƒë·∫ßu v√†o. |
| **Nh√¢n ma tr·∫≠n** | **V·∫•n ƒë·ªÅ:** ƒê·ªÉ nh√¢n ma tr·∫≠n trong PyTorch, c·∫ßn ƒë·∫£m b·∫£o ƒëi·ªÅu ki·ªán k√≠ch th∆∞·ªõc ph√π h·ª£p (`n x m` v√† `m x p`). | **Gi·∫£i ph√°p:** S·ª≠ d·ª•ng to√°n t·ª≠ `@` v√† `.T` (chuy·ªÉn v·ªã) ƒë·ªÉ th·ª±c hi·ªán `x_data @ x_data.T`, ƒë·∫£m b·∫£o ma tr·∫≠n c√≥ k√≠ch th∆∞·ªõc nh√¢n h·ª£p l·ªá. |

---

## VI. T√†i li·ªáu Tham kh·∫£o

1.  **PyTorch Documentation:** H∆∞·ªõng d·∫´n ch√≠nh th·ª©c v·ªÅ Tensor, Autograd v√† `torch.nn`.
